
#rule all_local: 
#   input: cov=expand("binning/{group}/C10K_coverage.tsv",group=GROUPS),
#          bins=expand("binning/{group}/clustering_gt500.csv",group=GROUPS)
#          touch=expand("binning/{group}/bin_creation.done",group=GROUPS)


#cut contigs by ORFs and get the bed file
rule cut_contigs:
    input:  fa="assembly/%s/{group}.fasta" % ASSEMBLER,
            gff="annotation/{group}/{group}.gff"
    output: contig="profile/split_c10k/{group}.fasta",
            Contig_bed="annotation/{group}/{group}_C10K_contig.bed"
    shell:  "{SCRIPTS}/Use_orf_to_cut.py {input.fa} {input.gff} > {output.contig}"


rule index_samples_bam:
    input:   "profile/{frags}/{group}/{sample}.sorted.bam",
    output:  "profile/{frags}/{group}/{sample}.sorted.bam.bai",
    message: "Indexing each bam file before coverage calcul"
    threads: THREADS
    shell:   "samtools index {input}"


# ---- use bedtool to compute coverage  ----------------------------------------------------
rule bedtools:
    input:   sample="profile/scaffolds/{group}/{sample}.sorted.bam",
             bed="annotation/{group}/{group}_C10K_contig.bed"
    output:  cov="profile/scaffolds/{group}/{sample}.cov",
    log:      "{group}/map/{sample}.log"
    shell:   "bedtools coverage -a {input.bed} -b {input.sample} -mean > {output} 2>{log} "

# ---- use a awk onliner to regroup all coverages into a unique file -----------------------
rule coverage:
    input:   lambda w : ["profile/scaffolds/"+w.group+"/"+sample.split('/')[-1]+".cov" for sample in  GROUPS[w.group]]
    output:  "binning/{group}/C10K_coverage.tsv"
    shell :  """
            echo -e "contig\t""$(ls {input} | cut -f1 -d "." | rev | cut -f1 -d "/" |rev | tr "\n" "\t" | sed 's/\t$//')"> {output}
            awk 'NR==FNR{{Matrix_coverage[1,FNR]=$4}}FNR==1{{f++}}{{Matrix_coverage[f+1,FNR]=$5}}END{{for(x=1;x<=FNR;x++){{for(y=1;y<ARGC+1;y++){{if(y<ARGC){{printf("%s\t",Matrix_coverage[y,x])}}if(y==ARGC){{printf("%s",Matrix_coverage[y,x]);print""}}}}}}}}' {input} >>{output}"""




# # use mosdepth to compute coverage by feature on a bed file
# rule depth_local:
#     input:   sample="profile/{frags}/{group}/{sample}.sorted.bam",
#              index="profile/{frags}/{group}/{sample}.sorted.bam.bai",
#              bed_file="annotation/{group}/{group}_C10K_contig.bed"
#     output:  cov="profile/{frags}/{group}/{sample}.cov",
#              gz_cov=temp("profile/{frags}/{group}/{sample}.regions.bed.gz")
#     log:     "profile/{frags}/{group}/{sample}.log"
#     message: "Calculating depths for {wildcards.frags} from {wildcards.group} across samples"
#     threads: min(THREADS,4)
#     shell:   """
#              mosdepth -t {threads} -n  -b {input.bed_file} profile/{wildcards.frags}/{wildcards.group}/{wildcards.sample} {input.sample}>{log}
#              zcat {output.gz_cov} | awk '{{print $5}}'>{output.cov}
#              zcat {output.gz_cov} | awk '{{print $4}}'>profile/{wildcards.frags}/{wildcards.group}/header.txt
#              rm profile/{wildcards.frags}/{wildcards.group}/{wildcards.sample}.mosdepth.global.dist.txt
#              rm profile/{wildcards.frags}/{wildcards.group}/{wildcards.sample}.mosdepth.region.dist.txt
#              rm profile/{wildcards.frags}/{wildcards.group}/{wildcards.sample}.regions.bed.gz.csi
#              """

# # merge all samples 
# rule merge:
#     input:   samples=expand("profile/scaffolds/{{group}}/{sample}.sorted.bam", sample=SAMPLES),
#              cov=expand("profile/scaffolds/{{group}}/{sample}.cov",sample=SAMPLES)
#     output:  cov="binning/{group}/C10K_coverage.tsv",
#              temp=temp("profile/scaffolds/{group}/{group}_temp.txt")
#     params:  tab='\t'.join(SAMPLES),
#              space=" ".join(map(lambda x:"profile/scaffolds/{group}/"+x+".cov",SAMPLES))
#     shell:   """
#              echo {params.space}
#              paste {params.space} >{output.temp}
#              echo -e 'contig\t'"{params.tab}">{output.cov}
#              paste profile/scaffolds/{wildcards.group}/header.txt {output.temp} >> {output.cov}
#              """

rule initial_quantity_of_bins:
    # we take a look at all the SCG, take the median over the 36 of them, and multiply that by 10
    input: "annotation/{group}/{group}_SCG.fna"
    output:"annotation/{group}/{group}_nb_ini_bins.txt"
    shell: "{SCRIPTS}/get_num_bin_ini.py {input}>{output}"

#  concoct 
rule concoct:
    input:   cov="binning/{group}/C10K_coverage.tsv",
             fasta="profile/split_c10k/{group}.fasta",
             nb_bin="annotation/{group}/{group}_nb_ini_bins.txt"
    output:  bins="binning/{group}/clustering.csv",
             data="binning/{group}/original_data.csv"
             #TODO simplify
    params:  bins="binning/{group}/clustering_gt%d.csv" % MIN_CONTIG_SIZE,
             data="binning/{group}/original_data_gt%d.csv" % MIN_CONTIG_SIZE
    log :   "binning/{group}/concoct.logs"
    threads: THREADS
    shell:   """
             concoct --coverage_file {input.cov} --composition_file {input.fasta} -b binning/{wildcards.group} -c $(<{input.nb_bin}) -l {MIN_CONTIG_SIZE} -t {threads} &>{log} 
             mv {params.bins} {output.bins} 
             mv {params.data} {output.data} 
             """

rule refine:
    input:  bins="binning/{group}/clustering.csv",
            SCG="annotation/{group}/{group}_SCG.fna",
            data="binning/{group}/original_data.csv"
    output: R=temp("binning/{group}/clustering_R.csv"),
            table="binning/{group}/clustering_SCG_table.csv",
            bins_R="binning/{group}/clustering_refine.csv",
            table_R="binning/{group}/clustering_SCG_table_R.csv"
    log:    temp("binning/{group}/clustering.log")
    threads: 1000
    shell:  """ {SCRIPTS}/SCG_in_Bins.py {input.bins} {input.SCG} -t {output.table}
            sed '1d' {input.bins}  > {output.R}
            cd binning/{wildcards.group}
            concoct_refine ../../{output.R} ../../{input.data} ../../{output.table} -t {threads} &>../../{log}
            cd ../../
            {SCRIPTS}/SCG_in_Bins.py {output.bins_R} {input.SCG} -t {output.table_R}
            """

rule merge_contigs:
    input:   "{path}/clustering_refine.csv"
    output:  "{path}/clustering_merged.csv"
    log:     "{path}/consensus.log"
    threads: THREADS
    shell:   "{SCRIPTS}/Consensus.py {input} >{output} 2>{log}"

rule create_bin_folders:
    input:   bin="binning/{group}/clustering_merged.csv",
             fasta="annotation/{group}/{group}_SCG.fna"
    output:  touch("binning/{group}/bin_folders.done")
    shell:   "{SCRIPTS}/SCG_in_Bins.py {input.bin} {input.fasta} -f binning/{wildcards.group}/"
