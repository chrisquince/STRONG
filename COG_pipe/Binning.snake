include: "Common.snake"
include: "Assembly.snake"
include: "cogs_annotation.snake"
#rule all_local: 
#   input: cov="binning/C10K_coverage.tsv",
#          bins="binning/clustering_gt500.csv",
#          touch="binning/bin_creation.done"


#TODO change paths?
#TODO check that works correctly with subset of samples?
#TODO switch to the gfa input?
rule unitig_profiles:
    input:
        graph="{path}.gfa",
        reads_desc="samples.yaml"
    output:
        "{path}.mult_prof"
    params:
        gp="{path}"
    log:
        "{path}.mult_prof.log"
    threads:
        THREADS
    shell:
        "{SOFT}/unitig-coverage {input.reads_desc} {input.graph} {output} "
        "-k {ASSEMBLY_K} -t {threads} --tmpdir $(dirname {output})/tmp &> {log}"

rule copy_fasta:
    input:   "assembly/%s/assembly.fasta" % ASSEMBLER
    output:  "profile/assembly.fasta"
    shell: "cp {input} {output}"


#cut contigs by ORFs and get the bed file
rule cut_contigs:
    input:  fa="profile/assembly.fasta",
            gff="annotation/assembly.gff"
    output: contig="profile/split.fasta",
            contig_bed="profile/split.bed"
    shell:  "{SCRIPTS}/Use_orf_to_cut.py {input.fa} {input.gff} {output.contig_bed} -c {FRAGMENT_SIZE} > {output.contig}"

# ---- use bedtool to compute coverage  ----------------------------------------------------
rule bedtools_split_cov:
    input:   sample="profile/assembly/{sample}.sorted.bam",
             bed="profile/split.bed"
    output:  "profile/split/{sample}.cov"
    log:     "profile/split/{sample}.log"
    shell:   "bedtools coverage -a {input.bed} -b {input.sample} -mean > {output} 2>{log} "

# ---- use a awk onliner to regroup all coverages into a unique file -----------------------
rule coverage:
    input:   expand("profile/split/{sample}.cov", sample = SAMPLES)
    output:  "profile/split/coverage.tsv"
    shell : """
            echo -e "contig\t""$(ls -U {input} | cut -f1 -d "." | rev | cut -f1 -d "/" |rev | tr "\n" "\t" | sed 's/\t$//')"> {output}
            awk 'NR==FNR{{Matrix_coverage[1,FNR]=$4}}FNR==1{{f++}}{{Matrix_coverage[f+1,FNR]=$5}}END{{for(x=1;x<=FNR;x++){{for(y=1;y<ARGC+1;y++){{if(y<ARGC){{printf("%s\t",Matrix_coverage[y,x])}}if(y==ARGC){{printf("%s",Matrix_coverage[y,x]);print""}}}}}}}}' {input} >>{output}"""

# We take a look at all the SCG, take the median over the 36 of them, and multiply that by 2
rule initial_quantity_of_bins:
    input: "annotation/SCG.fna"
    output:"binning/ini_bins.cnt"
    shell: "{SCRIPTS}/get_num_bin_ini.py {input}>{output}"

#  concoct 
rule concoct:
    input:   cov="profile/split/coverage.tsv",
             fasta="profile/split.fasta",
             bin_cnt="binning/ini_bins.cnt"
    output:  bins="binning/clustering_gt%d.csv" % MIN_CONTIG_SIZE,
             data="binning/original_data_gt%d.csv" % MIN_CONTIG_SIZE
    log :   "binning/concoct.logs"
    threads: 1000
    shell:   """
             concoct --coverage_file {input.cov} --composition_file {input.fasta} -b $(dirname {output.bins}) -c $(<{input.bin_cnt}) -l {MIN_CONTIG_SIZE} -t {threads} &>{log} 
             """

# generate orfs bed
rule bed_orfs:
    input:   gff="{path}/assembly.gff"
    output:  bed="{path}/assembly.bed"
    shell : "{SCRIPTS}/Gff_to_bed.py {input.gff} {output.bed}"


# path=binning
rule refine:
    input:  bins="{path}/clustering_gt%d.csv" % MIN_CONTIG_SIZE,
            SCG="annotation/SCG.fna",
            data="{path}/original_data_gt%d.csv" % MIN_CONTIG_SIZE,
            orf_bed = "annotation/assembly.bed",
            split_bed = "profile/split.bed"
    output: R=temp("{path}/clustering_gt%dR.csv") % MIN_CONTIG_SIZE,
            table="{path}/clustering_gt%d_SCG_table.csv" % MIN_CONTIG_SIZE,
            bins_R="{path}/clustering_refine.csv",
            table_R="{path}/clustering_gt%d_SCG_table_R.csv" % MIN_CONTIG_SIZE,
    log:    "{path}/refine.log"
    threads: 1000
    shell:  """ 
            {SCRIPTS}/SCG_in_Bins.py {input.bins} {input.SCG} {input.orf_bed} {input.split_bed} -t {output.table}
            sed '1d' {input.bins}  > {output.R}
            cd binning
            concoct_refine ../{output.R} ../{input.data} ../{output.table} -t {threads} &> ../{log}
            cd -
            {SCRIPTS}/SCG_in_Bins.py {output.bins_R} {input.SCG} {input.orf_bed} {input.split_bed} -t {output.table_R}
            """

rule merge_contigs:
    input:   "{path}/clustering_refine.csv",
    output:  "{path}/clustering_gt%d_merged.csv" % MIN_CONTIG_SIZE
    log:     "{path}/consensus.log"
    threads: THREADS
    shell:   "{SCRIPTS}/Consensus.py {input} >{output} 2>{log}"


rule create_bin_folders:
    input:   bin="binning/clustering_gt%d_merged.csv" % MIN_CONTIG_SIZE,
             fasta="annotation/SCG.fna",
             orf_bed = "annotation/assembly.bed",
             split_bed = "profile/split.bed"
    output:  mags="binning/list_mags.tsv"
    shell:   "{SCRIPTS}/SCG_in_Bins.py {input.bin} {input.fasta} {input.orf_bed} {input.split_bed} -all subgraphs/bin_init/ -l {output.mags}"

rule compute_avg_cov:
    input:   "binning/clustering_gt%d_merged.csv" % MIN_CONTIG_SIZE
    output:  "subgraphs/bin_init/bin_cov.tsv"
    shell:   "{SCRIPTS}/bin_cov.py {input} {output} {ASSEMBLY_K}"

