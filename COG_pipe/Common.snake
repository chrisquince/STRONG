configfile: "config.yaml"

from itertools import chain
from functools import partial
import os.path
import tempfile
import yaml

from scripts.common import detect_reads, fill_default_values

#Config parameters
fill_default_values(config)


# -------- Ressources ----------------
THREADS = config["threads"]
# -------- Data -----------------
IN = config["data"]
READ_LENGTH = config["read_length"]
# -------- Sources -----------------
SCRIPTS = config["scripts"]
DSCRIPTS = config["desman"]["dscripts"]
SOFT = config["soft"]
# -------- COGs -----------------
COG_DB= config["cog_database"]
SCG_DATA = config["scg_data"]
COG_FILE=SCG_DATA+"/scg_cogs_to_run.txt"
# -------- Assembly -----------------
ASSEMBLER = config["assembly"]["assembler"]
ASSEMBLER_DIR = config["assembly"]["dir"]
ASSEMBLY_K = config["assembly"]["k"][-1]
# -------- Bayesgraph -----------------
BAYESPATHS_DIR = config["bayespaths"]["dir"]
BAYESPATHS_G = config["bayespaths"]["nb_strains"]
# -------- Binning -----------------
FRAGMENT_SIZE = config["concoct_fragment_size"]
MIN_CONTIG_SIZE = config["concoct_contig_size"]
# -------- Desman -----------------
DESMAN_HAPLOTYPE_NB = config["desman"]["nb_haplotypes"]
DESMAN_REPEAT = config["desman"]["nb_repeat"]
MIN_COV_DESMAN = config["desman"]["min_cov"]
# -------- Evaluation -----------------
REFDATA=config["evaluation"]["genomes"]
EVAL_SCRIPTS=config["evaluation"]["scripts"]

#TODO rename + maybe simplify
LIST_COGS=list(map(str.rstrip,open(COG_FILE).readlines()))

#Autodetect samples and their reads
#Check that sample names are consecutive and all are presented
SAMPLE_DIRS = set(glob_wildcards(os.path.join(IN, "{sample,sample\d+}"))[0])
ALL_SAMPLES = list()
for i in range(1, len(SAMPLE_DIRS) + 1):
    sample_name = "sample" + str(i)
    if sample_name not in SAMPLE_DIRS:
        raise WorkflowError("Samples must be consecutive; missing " + sample_name)
    ALL_SAMPLES.append(sample_name)

SAMPLE_READS = { s : detect_reads(os.path.join(IN, s)) for s in ALL_SAMPLES}

def process_sample(i):
    if type(i) == int:
        res = "sample" + str(i)
    elif type(i) == str and i.startswith("sample"):
        res = i
    else:
        raise WorkflowException("Samples in groups must be named either sampleXX or XX, but got " + str(i))
    return res

def process_samples(g):
    if type(g) == list:
        res = list(map(process_sample, g))
    elif type(g) == str and g == "*": #Replace the wildcard group with unused samples
        res = ALL_SAMPLES
    else:
        raise WorkflowException("Groups must be either list of samples or a wildcard *, but got " + str(g))
    return res

#Samples to use
SAMPLES=process_samples(config["samples"])

#Helpers for locating input files
#Returns all filepaths with left/right reads for a sample/bin/etc, used as Snakemake input
    

def get_reads(read_type, wildcards):
    return SAMPLE_READS[wildcards["sample"]][read_type] 

left_reads = partial(get_reads, 0)
right_reads = partial(get_reads, 1)

def samples_yaml(samples):
    libs = []
    for s in samples:
        info = {}
        info["left reads"] = [SAMPLE_READS[s][0]]
        info["right reads"] = [SAMPLE_READS[s][1]]
        info["type"] = "paired-end"
        info["orientation"] = "fr"
        libs.append(info)

    return yaml.dump(libs, default_style='"', default_flow_style=False)

def is_fastq(wildcards):
    name = getattr(wildcards, "sample", None)
    if not name:
        try:
            name = GROUPS[wildcards.group][0]
        except KeyError:
            name = wildcards.group

    for ext in {".fastq", ".fq", ".fastq.gz", "fq.gz"}:
        if SAMPLE_READS[name][0].endswith(ext):
            return True
    return False


rule bowtie_index:
    input:   "{path}/{ref}.fasta"
    output:  touch("{path}/{ref}/index.done")
    params:  "{path}/{ref}/index"
    threads: THREADS
    log:     "{path}/{ref}/index.log"
    message: "Building bowtie index for {input}"
    shell:   "bowtie2-build {input} {params} --threads {THREADS} &> {log}"

#consider ignoring reads aligned without their pairs
rule bowtie_align:
    input:   left=left_reads, right=right_reads,
             index="{path}/index.done"
    output:  "{path}/{sample,sample\d+}.bam"
             #temp("{path}/{group,(sample|group)\d+}.bam")
    params:  flag=lambda w: "-q" if is_fastq(w) else "-f",
             left=lambda w: ",".join(expand("{l}", l=left_reads(w))),
             right=lambda w: ",".join(expand("{r}", r=right_reads(w))),
             index="{path}/index",
             align="",#"--no-unal --maxins 1000 --n-ceil 0,0 --np 100500",
             view=""#"-q 10"
    threads: THREADS
    log:     "{path}/{sample}.bowtie.log"
    message: "Aligning reads of {wildcards.sample} onto {params.index} with bowtie"
    shell:
        "bowtie2 -x {params.index} {params.flag} -p {threads} {params.align} -1 {params.left} -2 {params.right} 2> {log}"
        " | samtools view -bh {params.view} - > {output}"

rule samtools_sort:
    input:
        "{path}/{name}.bam"
    output:
        "{path}/{name}.sorted.bam"
        #temp("{path}/{name}.sorted.bam")
    threads: THREADS
    log:
        "{path}/{name}.sort.log"
    message: "Sorting {input}"
    run:
        shell("samtools sort --threads %d -T {wildcards.path}/{wildcards.name}.tmp -O bam -o {output} {input} &> {log}" % (threads - 1))

rule index_fasta:
    input: "{path}.fasta"
    output: "{path}.fasta.fai"
    shell: "samtools faidx {input}"

rule index_samples_bam:
    input:   "{path}.sorted.bam",
    output:  "{path}.sorted.bam.bai",
    message: "Indexing bam file {path}.sorted.bam"
    shell:   "samtools index {input}"
