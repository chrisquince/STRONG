include: "Common.snake"
import os
import glob
import matplotlib as mpl
mpl.use('Agg')
import numpy as np 
import matplotlib.pyplot as plt
from collections import defaultdict
from os.path import dirname, basename
from Bio.SeqIO.FastaIO import SimpleFastaParser as sfp


# will break if we change naming convention
SELECTED_MAGS = sorted([mag.rstrip() for mag in open("bayespaths/selected_bins.txt")],key=lambda x:int(x.split("_")[-1])+1000*("merged" in x))
# need to remove mags where all samples where ignored
SELECTED_MAGS = [mag for mag in SELECTED_MAGS if len(glob.glob("bayespaths/%s/*Haplo_*_path.txt"%mag))>0]
# need to remove mags when bayespath doesn't output haplotype path for any reason.
SELECTED_MAGS = [mag for mag in SELECTED_MAGS if os.stat(glob.glob('bayespaths/%s/*_Haplo_*_path.txt'%mag)[0]).st_size>0]



# for some rules, we also need to include mag constituting merged mags
MERGED_TO_MAGS = {line.rstrip().split("\t")[0]:line.rstrip().split("\t")[1:] for line in open("subgraphs/bin_init/bins_to_merge.tsv")}
SELECTED_MAGS2 = SELECTED_MAGS+ [mag for merged,mags in MERGED_TO_MAGS.items() for mag in mags if merged in SELECTED_MAGS]  


ALL_MAGS = {"Bin_%s"%line.rstrip() for line in open("binning/%s/list_mags.tsv"%BINNER)}

# there is the number of haplotype in the file name, this is hard for snakemake, so I need to get it beforehand
MAG_TO_PATHS = {mag:glob.glob("bayespaths/%s/*Haplo_*_path.txt"%mag)[0] for mag in SELECTED_MAGS}
MAG_TO_HAPLO = {mag:glob.glob("bayespaths/%s/*Haplo_*.fa"%mag)[0] for mag in SELECTED_MAGS}
MAG_TO_HAPLO_NB = {mag:len({header.split('_')[1] for header,_ in sfp(open(file))}) for mag,file in MAG_TO_PATHS.items()}

# number of cogs is dependant on mags 
MAG_TO_COGS = {mag:sorted({header.split('_')[0] for header,seq in sfp(open(MAG_TO_PATHS[mag]))}) for mag in SELECTED_MAGS}

# evaluation results
if EVAL_RUN == 1:
    MAG_TO_STRAINS = {mag:[line.split(",")[0] for line in open("evaluation/bayespaths/%s_combine.tsv"%mag)] for mag in SELECTED_MAGS}
else:
    MAG_TO_STRAINS = {mag:[] for mag in SELECTED_MAGS}

# get strong env path..... for reason
def which(pgm):
    # https://stackoverflow.com/questions/9877462/is-there-a-python-equivalent-to-the-which-command
    path=os.getenv('PATH')
    for p in path.split(os.path.pathsep):
        p=os.path.join(p,pgm)
        if os.path.exists(p) and os.access(p,os.X_OK):
            return p

ENV_PATH = dirname(dirname(which("concoct")))
#--------------------Snakemake ------------------------


#### specify wanted outputs
# summary file :
TODO_LIST = ["results/summary.tsv"]

# generate colored SCG graphs, one by mag
TODO_LIST.append(expand("results/{mag}/graph/joined_SCG_graph.gfa",mag=SELECTED_MAGS))

# generate haplotype tree, one for each mag
TODO_LIST.append(expand("results/{mag}/haplotypes_tree.nwk",mag=SELECTED_MAGS))
TODO_LIST.append("results/haplotypes_tree.pdf")

# generate haplotype coverage
TODO_LIST.append("results/haplotypes_coverage.pdf")
TODO_LIST.append("results/mag_and_haplo_cov.tsv")

# fig : haplo_vs_magcov
TODO_LIST.append("results/haplo_nb_vs_mag_cov.png")

# if GTDB is not empty, generate a gtdb tree of mags 
if GTDB!="":
    TODO_LIST += ["results/gtdb/gtdbtk.bac120.summary.tsv"]

# Additional stat on percent readss mapped
TODO_LIST += ["profile/mapping_percent.tsv"]

rule all: 
    input: TODO_LIST

# ----------------- global summary -----------------
# Adress the following --> nb_strains, coverage of each strains, total mag coverage, divergence, nb mags, nb bins, quality of assembly  

def get_taxa(file,mag_to_classification={}):
    if os.path.getsize(file)!=0:
        with open(file) as handle : 
            header = next(handle)
            for line in handle :
                splitline = line.rstrip().split("\t")
                mag = splitline[0]
                classification = [ taxa.split("__")[1] for taxa in splitline[1].split(';')]
                classification = [taxa if taxa else '/' for taxa in classification]
                Red = splitline[-2]
                if Red == 'N/A' :
                    Red = 1
                mag_to_classification[mag] = "\t".join([str(Red)]+classification)
    return mag_to_classification

generate_summary_input = ["profile/assembly.fasta"]+(GTDB!="")*["results/gtdb/gtdbtk.bac120.summary.tsv","results/gtdb/gtdbtk.ar122.summary.tsv"]

rule generate_summary:
    input: generate_summary_input
    output: file = 'results/summary.tsv'
    run:
        # information on assembly
        with open(output["file"],"w") as handle:
            handle.write("#-------- assembly statistics --------\n")
        shell("{RES_SCRIPTS}/contig-stats.pl %s>>{output}"%input[0])
        handle = open(output["file"],"a")
        # information on mags
        handle.write("#--------     MAGs summary    --------\n")
        mag_nb = len(ALL_MAGS)
        haplo_nb = sum(MAG_TO_HAPLO_NB.values())
        handle.write("mags number: %s\tresolved haplotypes: %s\n"%(mag_nb,haplo_nb))
        if GTDB:
            bac_nb = sum([1 for line in open(input[1])])
            arc_nb = sum([1 for line in open(input[2])])
            handle.write("bacteria: %s\tarcheae: %s\n"%(bac_nb,arc_nb))
            # add taxonomy and RED value
            handle.write("#-------- MAGs gtdb taxonomy  --------\n")
            handle.write("mag\tnb_haplo\tRED\tDomain\tPhylum\tClass\tOrder\tfamilly\tgenus\tstrain\n")
            mag_to_taxa = get_taxa(input[1])
            mag_to_taxa = get_taxa(input[2]) # worst non readable trick I know so far, using an empty dict as defaultvalue will cary over multiple call.... https://stackoverflow.com/questions/26320899/why-is-the-empty-dictionary-a-dangerous-default-value-in-python 
            for mag,taxo in mag_to_taxa.items():
                if mag in MAG_TO_HAPLO_NB:
                    haplo_nb = MAG_TO_HAPLO_NB[mag]
                else: 
                    haplo_nb = "NA"
                handle.write("%s\t%s\t%s\n"%(mag,haplo_nb,taxo))

# ----------------- coverage of mag/strains -----------------
##### get SCG coverages
# keep only cogs corresponding to scg
rule get_scg_coverage:
    input:   bed = "annotation/assembly.bed",
             scg_annot = "annotation/Cogs_filtered.tsv"
    output:  bed="annotation/scg.bed",
             scg_to_orfs = "annotation/map_scgs_to_orfs.tsv"
    run : 
        scgs = {line.rstrip() for line in open(COG_FILE)}
        # get scg to orfs mapping
        scg_to_orfs = defaultdict(list)
        for line in open(input["scg_annot"]):
            splitlines = line.split("\t")
            if splitlines[1] in scgs:
                scg_to_orfs[splitlines[1]].append(splitlines[0])
        # select only orfs relevant to scg cogs
        orfs = {val for values in scg_to_orfs.values() for val in values}
        with open(output["bed"],"w") as handle:
            for line in open(input["bed"]):
                if line.rstrip().split('\t')[3] in orfs:
                    handle.write(line)
        # output scg to orf mapping : 
        with open(output["scg_to_orfs"],"w") as handle:
            handle.writelines("%s\t%s\n"%(scg,"\t".join(orfs)) for scg,orfs in scg_to_orfs.items())

# extract cov for selected features
rule bedtools:
    input:   bam="profile/assembly/{sample}.sorted.bam",
             bed="annotation/scg.bed"
    output:  "profile/scgs/{sample}.orfs.cov"
    log:      "profile/scgs/{sample}.orfs.log"
    resources:
        memG=400
    shell:   "bedtools coverage -a {input.bed} -b {input.bam} -mean > {output} 2>{log} "

# collate all files  
rule coverage:
    input:   expand("profile/scgs/{sample}.orfs.cov",sample = SAMPLES)
    output:  "profile/coverage_scg_orfs.tsv"
    shell : "{SCRIPTS}/collate_coverage.py -o {output} -l {input} "

# aggregate orf coverage following inputed scg to orfs mapping
rule generate_profile:
    input:  cov="profile/coverage_scg_orfs.tsv",
            map="annotation/map_scgs_to_orfs.tsv"
    output: "profile/cov_scg.tsv"
    shell:  "{SCRIPTS}/Extract_gene_profile.py {input.map} {input.cov} {output}"

# just take the median of cov_scg.tsv
rule median_SCG_cov:
    input:  cov = "profile/cov_scg.tsv",
            nuc = "profile/nucleotides.tsv"
    output: "profile/Normalisation.tsv"
    run: 
        List_profile = []

        # compute median of SCG       
        with open(input.cov) as handle:
            sorted_samples = next(handle).rstrip().split("\t")[1:]
            List_profile = [[float(element) for element in line.rstrip().split("\t")[1:]] for line in handle]
        scg_norm=np.median(List_profile, axis=0)

        # get nucleotides
        nuc,_,_ = load_matrix(input["nuc"],sample_order=sorted_samples)

        # output
        with open(output[0],"w") as handle:
            handle.write("Normalisation\t"+"\t".join(sorted_samples)+"\n")
            handle.write("median_scg\t"+"\t".join(map(str,scg_norm))+"\n")
            handle.write("nucleotides\t"+"\t".join(map(str,nuc[0,:]))+"\n")

#### get selected all mag coverage
rule mag_coverage:
    input: cluster = "subgraphs/bin_merged/clustering.csv",
           cov = "profile/split/coverage.tsv",
           bed = "profile/split.bed",
           norm = "profile/Normalisation.tsv",
    output: cov = "profile/mag_cov.tsv",
            p_map = "profile/mag_percent_mapped.tsv"
    shell : "{RES_SCRIPTS}/mag_coverage_from_splitcontigs.py {input.cluster} {input.cov} {input.bed} {input.norm} {output.cov} {output.p_map}"



def format_row(header,line):
    row = header
    for el in line:
        if np.isnan(el):
            row += "\tNA"
        else:
            row += "\t{:.2g}".format(el)
    return row+"\n"
def matrix_write(matrix,file_name,col_names,row_names):
    with open(file_name,"w") as handle:
        handle.write("/\t%s\n"%"\t".join(col_names))
        handle.writelines(format_row(row_names[index],line) for index,line in enumerate(matrix))

def load_matrix(file,sample_order=None,strain_order=None):
    with open(file) as handle :
        header = next(handle).rstrip().split("\t")[1:]
        strains = []
        matrix = []
        for line in handle : 
            splitlines = line.rstrip().split("\t")
            strains.append(splitlines[0])
            matrix.append(list(map(float,splitlines[1:])))
    matrix = np.array(matrix)
    if sample_order:
        reorder_samples = [header.index(sample) for sample in sample_order]
        matrix = matrix[:,reorder_samples]
        header = sample_order
    if strain_order:
        reorder_strain = [strains.index(strain) for strain in strain_order]
        matrix = matrix[reorder_strain,:]
        strains = strain_order
    return matrix,header,strains

def convert_intensity_to_kcov(mat):
    return mat*READ_LENGTH

def convert_kcov_to_cov(mat):
#Ck*Read_length = C *(Read_length-kmer_size+1)
    return mat*READ_LENGTH/float(READ_LENGTH-ASSEMBLY_K+1)

def load_intensity(file,mag,norm):
    with open(file) as handle:
        header = [int(val) for val in next(handle).rstrip().split(",")[1:]]
        sorted_haplo = [] 
        matrix = []
        NA_terms = [index for index in range(len(SAMPLES)) if index not in header]
        for line in handle: 
            temp = [0 for ite in range(len(SAMPLES))]
            splitlines = line.rstrip().split(",")
            sorted_haplo.append("%s_haplo_%s"%(mag,splitlines[0]))
            for index,val in enumerate(splitlines[1:]):
                temp[header[index]]=float(val)
            matrix.append(temp)
    matrix = np.array(matrix)
    matrix = convert_intensity_to_kcov(matrix)
    matrix = convert_kcov_to_cov(matrix)
    matrixN = matrix/norm
    matrixNA = matrixN
    matrixNA[:,NA_terms]=np.nan
    return matrixNA,sorted_haplo


# get one matrix with mag coverage and strains coverages
rule create_unique_coverage_matrix:
    input: cov = "profile/mag_cov.tsv",
           norm = "profile/Normalisation.tsv"
    output: cov = 'results/mag_and_haplo_cov.tsv'
    run:
        mag_matrix,samples,mags = load_matrix(input["cov"])
        norm,_,_ = load_matrix(input["norm"],sample_order=samples)
        norm = norm[0,:]
        mag_matrixN = mag_matrix/norm
        with open(output["cov"],"w") as handle: 
            handle.write("/\t%s\n"%"\t".join(samples))
            for mag in SELECTED_MAGS:
                handle.write(format_row(mag,mag_matrixN[mags.index(mag)]))
                file = "bayespaths/%s/%sF_Intensity.csv"%(mag,mag) 
                haplo_matN,sorted_haplo = load_intensity(file,mag,norm)
                for index,line in enumerate(haplo_matN):
                    handle.write(format_row(sorted_haplo[index],line))


# --------------plot nb strain VS total mag coverage --------------
rule plot_fig :
    input : cov = "profile/mag_cov.tsv"
    output: fig = "results/haplo_nb_vs_mag_cov.png"
    run: 
        with open("results/mag_to_haplo_nb.tsv","w") as handle:
            handle.writelines("%s\t%s\n"%(mag,nb) for mag,nb in MAG_TO_HAPLO_NB.items())
        shell("{RES_SCRIPTS}/plot_mag_to_nb.R {input} results/mag_to_haplo_nb.tsv {output}" )
        shell("rm results/mag_to_haplo_nb.tsv")

# ----------------- barplot strain coverage -----------------
rule coverage_barplot:
    input: cov = "bayespaths/{mag}/{mag}F_Intensity.csv",
           var = "bayespaths/{mag}/{mag}F_varIntensity.csv",
           norm = "profile/Normalisation.tsv"
    output: "results/{mag}/haplotypes_coverage.pdf"
    shell: "{RES_SCRIPTS}/GGBarPlotAbundances.R {input.cov} {input.var} {input.norm} {READ_LENGTH} {ASSEMBLY_K} {output}"

rule cat_pdf:
    input: expand("results/{mag}/haplotypes_coverage.pdf",mag=SELECTED_MAGS)
    output: "results/haplotypes_coverage.pdf"
    shell: "pdfunite {input} {output}"


# ----------------- haplotype reanotation -----------------
# due to prior step, if an orf is completely on a well resolved contig, the whole contig is outputed.
# the same contig can be outputed for multiple cogs and even if the contig is of size simillar to the cog considered, there is no guaranty that its start and end are the start and end of the cog. So let's just run prodigal/rpsblast on thoses
rule reduced_fasta:
    output:file = "results/{mag}/tmp/annotation/haplo_unique.fa" 
    run:
        mag = wildcards.mag
        seq_to_cog = defaultdict(list)
        for header,seq in sfp(open(MAG_TO_HAPLO[mag])):
            seq_to_cog[seq].append(header)
        with open(output["file"],"w") as handle:
            handle.writelines(">%s\n%s\n"%("|".join(headers),seq) for seq,headers in seq_to_cog.items())

# don't try to run -p single, it removes some SCG.
rule prodigal:
    input: "results/{mag}/tmp/annotation/haplo_unique.fa" 
    output:
        faa="results/{mag}/tmp/annotation/haplo.faa",
        fna="results/{mag}/tmp/annotation/haplo.fna",
        gff="results/{mag}/tmp/annotation/haplo.gff"
    log: "results/{mag}/tmp/prodigal.log"
    shell : "prodigal -i {input} -a {output.faa} -d {output.fna} -p meta -f gff -o {output.gff} &> {log} "

rule rpsblast:
    input: faa = "{path}/{name}.faa"
    output: "{path}/{name}_Rpsblast_cogs.tsv"
    log: "{path}/{name}_Rpsblast_cogs.log"
    shell: "rpsblast -outfmt '6 qseqid sseqid evalue pident length slen qlen' -query {input.faa} -db {COG_DB} -out {output}"

rule parse_cogs_annotation:
    input:  "{path}/{name}_Rpsblast_cogs.tsv"    
    output: "{path}/{name}_Cogs_filtered.tsv"
    log:    "{path}/{name}_cog_filtering.log"
    shell:  "{SCRIPTS}/Filter_Cogs.py {input} --cdd_cog_file {SCG_DATA}/cdd_to_cog.tsv  > {output} 2> {log}"

rule extract_cog_sequence:
    input:  fa = "results/{mag}/tmp/annotation/haplo_unique.fa",
            fna = "results/{mag}/tmp/annotation/haplo.fna",
            cog = "results/{mag}/tmp/annotation/haplo_Cogs_filtered.tsv"
    output: out = "results/{mag}/haplotypes_cogs.fna"
    shell: "{RES_SCRIPTS}/haplotypes_to_cogs.py {input.fa} {input.fna} {input.cog} {output.out}"

# ----------------- database cog_annotation -----------------
rule move_db_seq_to_folder:
    output: "results/gtdb/refs/{ref}/{ref}.fa"
    run: 
        ref = wildcards["ref"]
        path = next(line for line in open("%s/fastani/genome_paths.tsv"%GTDB)if ref in line).rstrip().split()
        shell("gunzip -c %s/fastani/%s%s > {output}"%(GTDB,path[1],path[0]))

# gtdb and eval
rule prodigal_gtdb:
    input: "{path}/{ref}/{ref}.fa"
    output:
        faa="{path}/{ref}/{ref}.faa",
        fna="{path}/{ref}/{ref}.fna",
        gff="{path}/{ref}/{ref}.gff"
    log: "{path}/{ref}/prodigal.log"
    shell : "prodigal -i {input} -a {output.faa} -d {output.fna} -p single -f gff -o {output.gff} &> {log} "

rule prodigal_eval:
    input: "%s/Genomes/{ref}seq.tmp"%REFDATA
    output:
        faa="%s/cogs/{ref}/{ref}.faa"%REFDATA,
        fna="%s/cogs/{ref}/{ref}.fna"%REFDATA,
        gff="%s/cogs/{ref}/{ref}.gff"%REFDATA
    log: "%s/cogs/{ref}/prodigal.log"%REFDATA
    shell : "prodigal -i {input} -a {output.faa} -d {output.fna} -p single -f gff -o {output.gff} &> {log} "


rule extract_SCG_sequences:
    input:
        annotation="{path}/{ref}/{ref}_Cogs_filtered.tsv",
        gff="{path}/{ref}/{ref}.gff",
        fna="{path}/{ref}/{ref}.fna"
    output:
        "{path}/{ref}/SCG.fna"
    shell:
        "{SCRIPTS}/Extract_SCG.py {input.fna} {input.annotation} {COG_FILE} {input.gff} > {output}"


def get_all_refs(w):
    _ = checkpoints.gtdb.get()
    bac = [line.rstrip().split("\t")[7] for index,line in enumerate(open("results/gtdb/gtdbtk.bac120.summary.tsv")) if index>0 if line.rstrip().split("\t")[0] in SELECTED_MAGS2]
    ar  = [line.rstrip().split("\t")[7] for index,line in enumerate(open("results/gtdb/gtdbtk.ar122.summary.tsv")) if index>0 if line.rstrip().split("\t")[0] in SELECTED_MAGS2]
    return ["results/gtdb/refs/%s/SCG.fna"%name for name in bac+ar if name!="N/A"]




rule link_gtdb_to_folder:
    input: refs = get_all_refs,
           bac = "results/gtdb/gtdbtk.bac120.summary.tsv",
           ar  = "results/gtdb/gtdbtk.ar122.summary.tsv",
           merged = "subgraphs/bin_init/bins_to_merge.tsv"
    output: ["results/%s/tmp/gtdb_cogs.fna"%mag for mag in SELECTED_MAGS]
    run:
        mag_to_ref = {line.rstrip().split("\t")[0]:[line.rstrip().split("\t")[7]] for file in [input["bac"],input["ar"]] for line in open(file)}
        mag_to_ref = {mag:ref for mag,ref in mag_to_ref.items() if ref!=["N/A"]}
        # add refs from merged mags
        merged_to_mag = {line.rstrip().split("\t")[0]:line.rstrip().split("\t")[1:] for line in open(input["merged"])}
        # remove NA from merged_to_mag :
        merged_to_mag = {merged:[mag for mag in mags if mag in mag_to_ref] for merged,mags in merged_to_mag.items()}
        merged_to_mag = {key:values for key,values in merged_to_mag.items() if values}
        mag_to_ref.update({merged:[mag_to_ref[mag][0] for mag in mags] for merged,mags in merged_to_mag.items()})
        mag_to_ref = {mag:refs for mag,refs in mag_to_ref.items() if mag in SELECTED_MAGS}
        for mag,refs in mag_to_ref.items():
            with open("results/%s/tmp/gtdb_cogs.fna"%mag,"w") as handle:
                for ref in refs:
                    ref_scg = "results/gtdb/refs/%s/SCG.fna"%ref
                    handle.writelines(">%s|%s\n%s\n"%(ref,header,seq) for header,seq in sfp(open(ref_scg)))
        for file in output:
            shell("touch %s"%file)

rule get_eval_scg:
    input: refs = expand("%s/cogs/{ref}/SCG.fna"%REFDATA,ref={ref for refs in MAG_TO_STRAINS.values() for ref in refs})
    output: ["results/%s/tmp/eval_cogs.fna"%mag for mag in SELECTED_MAGS]
    run:
        for mag,refs in MAG_TO_STRAINS.items():
            with open("results/%s/tmp/eval_cogs.fna"%mag,"w") as handle:
                for ref in refs:
                    ref_scg = "%s/cogs/%s/SCG.fna"%(REFDATA,ref)
                    handle.writelines(">%s|%s\n%s\n"%(ref,header,seq) for header,seq in sfp(open(ref_scg)))


# ----------------- one denovo tree by mag -----------------
def add_seq_to_dict(mag,seq,mag_to_seq):
    # there meay be contamination or fragment, so multiple cogs for 1 mag, keep all version and name them differently
    if mag in mag_to_seq:
        if "_nb_" in mag :
            name,counter = mag.split("_nb_")
            counter = int(counter)+1
            mag = "%s_nb_%s"%(name,counter)
            return add_seq_to_dict(mag,seq,mag_to_seq)
        else : 
            mag = mag+"_nb_2"
            return add_seq_to_dict(mag,seq,mag_to_seq)
    else:
        mag_to_seq[mag] = seq
        return mag



split_by_cog_input =[expand("results/{mag}/haplotypes_cogs.fna",mag=SELECTED_MAGS)]+(GTDB!="")*[expand("results/{mag}/tmp/gtdb_cogs.fna",mag=SELECTED_MAGS)] +(REFDATA!="")*[expand("results/{mag}/tmp/eval_cogs.fna",mag=SELECTED_MAGS)]

checkpoint split_by_cog:
    input : split_by_cog_input
    output: mag_cogs = temp("results/mag_to_cogs.tsv"),
            map = expand("results/{mag}/tmp/map_magseq_to_orf.tsv", mag=SELECTED_MAGS)
    run : 
        # need to recompute mag_to_cogs since sometimes there can be missing ones from rpsblast annotation
        mag_to_cogs = defaultdict(list)
        # for each selected mag get the low resolution SCG sequence
        for file in ["results/%s/haplotypes_cogs.fna"%mag for mag in SELECTED_MAGS]:
            mag = basename(dirname(file))
            #### get resolved haplotypes sequences
            cog_to_strain_seq = defaultdict(lambda:defaultdict(list))
            for header,seq in sfp(open(file)):
                cog,strain_nb,cog_nb = header.split("_")
                mag_to_cogs[mag].append(cog)
                strain = "%s_haplo_%s"%(mag,strain_nb)
                cog_to_strain_seq[cog][strain].append([seq,int(cog_nb.replace("nb",""))])
            # cat sequences with NNNNN
            for cog,strain_to_seq in cog_to_strain_seq.items():
                for strain,seqs in strain_to_seq.items():
                    new_seq = "NNNNN".join([seq for seq,index in sorted(seqs,key=lambda x:x[1])])
                    cog_to_strain_seq[cog][strain] = new_seq

            #### get the low resolution SCG sequence
            file2 = "subgraphs/bin_merged/%s/SCG.fna"%mag
            header_to_name = {}
            for header,seq in sfp(open(file2)):
                cog = header.split(" ")[1]
                name = add_seq_to_dict(mag,seq,cog_to_strain_seq[cog])
                header_to_name[header] = name
            #### get gtdb sequences
            if GTDB:
                file3 = "results/%s/tmp/gtdb_cogs.fna"%mag
                for header,seq in sfp(open(file3)):
                    cog = header.split(" ")[1]
                    name = 'gtdb_%s'%header.split("|")[0]
                    cog_to_strain_seq[cog][name] = seq
            #### get gtdb sequences
            if REFDATA:
                file4 = "results/%s/tmp/eval_cogs.fna"%mag
                for header,seq in sfp(open(file4)):
                    cog = header.split(" ")[1]
                    name = 'eval_%s'%header.split("|")[0]
                    cog_to_strain_seq[cog][name] = seq

            #### output all sequences
            for cog,strain_to_seq in cog_to_strain_seq.items(): 
                with open("results/%s/tmp/%s.fna"%(mag,cog),"w") as handle:
                    handle.writelines(">%s\n%s\n"%(strain,seq) for strain,seq in strain_to_seq.items())
            #### output header to new name
            with open("results/%s/tmp/map_magseq_to_orf.tsv"%mag,'w') as handle:
                handle.writelines("%s\t%s\n"%(header,name) for header,name in header_to_name.items())
        #### output mapping of mags to cogs
        with open(output["mag_cogs"],'w') as handle:
            handle.writelines("%s\t%s\n"%(mag,"\t".join(cogs)) for mag,cogs in mag_to_cogs.items())


rule run_mafft:
    input: scg="results/{mag}/tmp/COG{cog}.fna",
    output: msa="results/{mag}/tmp/COG{cog}.msa",
    log: "results/{mag}/tmp/COG{cog}_mafft.log"
    threads:1
    shell: """
    mafft --thread {threads} {input.scg} > {output.msa} 2>{log}
    """

rule trimal :
    input: "{path}/{COG}.msa"
    output: "{path}/{COG}_trim.msa"
    shell: "trimal -in {input} -out {output} -gt 0.9 -cons 60"


rule COG_dist:
    input: "{path}/{COG}_trim.msa"
    output:"{path}/{COG}_dist.tsv"
    log:"{path}/{COG}_dist.log"
    run: # better than doing a checkpoint
        nb = open(input[0]).read().count(">")
        if nb>1:
            shell("distmat -nucmethod 0 {input} -outfile {output} &>{log}")
        else:
            shell("touch {output}")


def cat_cogs_inputs(w):
    file = checkpoints.split_by_cog.get().output["mag_cogs"]
    mag_to_cogs = {line.split("\t")[0]:line.rstrip().split("\t")[1:] for line in open(file)}
    alignement_trimmed = ["results/%s/tmp/%s_trim.msa"%(w.mag,cog) for cog in mag_to_cogs[w.mag]]
    alignement_distances = ["results/%s/tmp/%s_dist_mat.tsv"%(w.mag,cog) for cog in mag_to_cogs[w.mag]]
    return alignement_trimmed+alignement_distances

rule cat_cogs :
    input : cat_cogs_inputs,
            scg_cov = "profile/coverage_scg_orfs.tsv",
            map_names = "results/{mag}/tmp/map_magseq_to_orf.tsv"
    output : cat = "results/{mag}/tmp/concatenated_cogs.msa"
# super brittle script : input.trim and input.dist are globed inside, from expected path.
# will break if trim and dist path change,
# will break if mag names convention changes (has Bin_ inside, and not haplo, has _nb_ )
# will break if haplotype names convention changes
    shell: "{RES_SCRIPTS}/cat_msa_sequences.py {input.scg_cov} {input.map_names} {output.cat}"
        

rule Launch_FastTree:
    input: "{path}/tmp/concatenated_cogs.msa"
    output: "{path}/haplotypes_tree.nwk"
    log: "{path}/tmp/fastree.log"
    shell:"""
    FastTreeMP -nt -gtr < {input} 2> {log} > {output}
    """

#--- look at dist between strains ---
rule concatenated_seq_dist:
    input: "{path}/concatenated_cogs.msa"
    output:"{path}/dist.tsv"
    log:"{path}/dist.log"
    shell: "distmat -nucmethod 0 {input} -outfile {output} &>{log}" 

rule parse_emboss_dist:
    input: dist = "{path}dist.tsv"
    output: dist = "{path}dist_mat.tsv"
    run:
        if open(input["dist"]).read()=="":
            # sometimes there is only 1 seq for a cog, we could use a checkpoint, to unschedule such occurence, but it's super heavy dagwise. 
            # so let's just not use checkpoints and go on when a file is empty
            shell("touch {output.dist}")
        else:
            with open(input["dist"]) as handle:
                for nb in range(8):
                    header=next(handle).rstrip().replace(" ","").split("\t")
                nb_genomes=len(header)-header.count("")
                Mat=np.zeros((nb_genomes,nb_genomes))
                sorted_gen=[]
                for ind,line in enumerate(handle):
                    splitline=line.rstrip().split("\t")[1:]
                    genome=splitline[-1].split(" ")[0]
                    sorted_gen.append(genome)
                    Mat[ind,ind:]=np.array([float(el) for el in splitline[ind:-2]])
                Mat+=Mat.T
            matrix_write(Mat,output["dist"],sorted_gen,sorted_gen)



#--- plot tree + dist between strain ---
rule plot_tree_fig:
    input: dist = "{path}/tmp/dist_mat.tsv",
           tree = "{path}/haplotypes_tree.nwk"
    output: "{path}/haplotypes_tree.pdf"
    log : "{path}/tmp/haplotypes_tree.log"
    shell: "{RES_SCRIPTS}/plot_scg_tree.R {input.tree} {input.dist} {output} &>{log}"

rule cat_tree_pdf:
    input: expand("results/{mag}/haplotypes_tree.pdf",mag=SELECTED_MAGS)
    output: "results/haplotypes_tree.pdf"
    shell: "pdfunite {input} {output}"



# ----------------- colored .gfa -----------------
# keep the same color for the same strain --> generate 1 graph by mag by cog
rule color_graph :
    input : gfa = "subgraphs/bin_merged/{mag}/simplif/{cog}.gfa",
            path = lambda w:MAG_TO_PATHS[w.mag]
    output : "results/{mag}/graph/cogs/{cog}_color.gfa"
    params : kmer = config["assembly"]["k"]
    shell : """
    {RES_SCRIPTS}/color_graph.py {input.gfa} -p {input.path} {output}.temp
    {SCRIPTS}/Bandage_Cov_correction.py {output}.temp {params.kmer} {output}
    rm {output}.temp
    """

# also merge .gfa so that we can see some sort of continuity
rule join_graphs :
    input : path = lambda w:MAG_TO_PATHS[w.mag],
            gfa = lambda w:["results/%s/graph/cogs/%s_color.gfa"%(w.mag,cog) for cog in  MAG_TO_COGS[w.mag]]
    output : "results/{mag}/graph/joined_SCG_graph.gfa"
    shell : "{RES_SCRIPTS}/join_graphs.py {input.path} {output} -l {input.gfa}"


# ----------------- gtdb tree  -----------------
rule mag_directory:
    input: clustering = "binning/{binner}/clustering_{binner}.csv",
           assembly = "profile/assembly.fasta",
           mag_list = "binning/{binner}/list_mags.tsv"
    output: done = "binning/{binner}/MAGs/done"
    shell: """
           {SCRIPTS}/Split_fasta_by_bin.py {input.assembly} {input.clustering} $(dirname {output.done}) -l $(cat {input.mag_list})
           touch {output.done}
           """


rule get_gtdb_database:
    output: db_done="results/gtdb/done",
    log: "results/gtdb/download.log"
    run:
        # get strong env path
        path = which("gtdbtk")
        env_path = path.split("/bin/gtdbtk")[0]
        # just check it is not empty
        if len(os.listdir(GTDB))<=1:
            # create a copy of download script with new dl directory
            dl_script = which("download-db.sh")
            new_dl_script = dirname(dl_script)+"/download-db2.sh"
            shell("cp %s %s"%(dl_script,new_dl_script))
            shell("sed -i 's=${{GTDBTK_DATA_PATH}}=%s=g' %s"%(GTDB,new_dl_script))
            shell("download-db2.sh &>{log}")
        # overwrite default database location
        export_path = env_path+"/etc/conda/activate.d/gtdbtk.sh"
        with open(export_path,"w") as handle:
            handle.write("export GTDBTK_DATA_PATH=%s/\n"%GTDB)
        # signal process went is done
        shell("touch %s"%output["db_done"])

checkpoint gtdb:
    input: done = "binning/%s/MAGs/done"%BINNER,
           db_done = 'results/gtdb/done'
    output: bac = "results/gtdb/gtdbtk.bac120.summary.tsv",
            arc = "results/gtdb/gtdbtk.ar122.summary.tsv"
    log: "results/gtdb/logs"
    priority: 50
    threads: 50
    shell: """
            export "GTDBTK_DATA_PATH={GTDB}/"
            gtdbtk classify_wf --cpus {threads} --skip_ani_screen --mash_db --genome_dir $(dirname {input.done})  --out_dir $(dirname {output.bac}) --extension .fasta &> {log}
            touch {output.bac}
            touch {output.arc}
           """

# ----------------- visual output gtdb  -----------------
#TODO


# ----------------- Add nucleotide in normalisation  -----------------
rule nb_nuc:
    output: temp("profile/{sample}_nb_nuc.tsv")
    params: lambda x:R1[x.sample]
    shell: "seqtk fqchk {params}| grep ALL | cut -f2 > {output}"

rule nb_bases:
    input: expand("profile/{sample}_nb_nuc.tsv",sample=SAMPLES)
    output: temp("profile/nucleotides.tsv")
    run: 
        print(SAMPLES)
        dict_sample_nb=defaultdict(list)
        for file in input:
            sample = basename(file).split("_nb_nuc.tsv")[0]
            with open(file) as handle:
                nb = next(handle).rstrip()
                dict_sample_nb[sample].append(float(nb))
        dict_sample_nb = {sample:str(sum(list_nb)) for sample,list_nb in dict_sample_nb.items()}
        sorted_samples = sorted(dict_sample_nb.keys())
        with open(output[0],"w") as handle:
            handle.write("Normalisation\t%s\n"%"\t".join(sorted_samples))
            handle.write("Nb_nucleotides\t%s\n"%"\t".join([dict_sample_nb[sample] for sample in sorted_samples]))



# ----------------- Add percent reads mapped to assembly -----------------
rule mapped_reads:
    input:  bam="{path}/{sample}.sorted.bam"
    output: readnb="{path}/{sample}_mapped_read.txt"
    threads: 2
    shell: """samtools flagstat {input.bam} -@ {threads} | grep 'mapped (' | cut -f1 -d " " > {output.readnb}""" 

rule reads_qty:
    input:  R1=lambda w:R1[w["sample"]]
    output: "{path}/{sample}/{sample}_readsnb.txt"
    shell: "echo $(($(zcat {input.R1} | wc -l)/2)) > {output}"

rule get_percent:
    input: mapped = expand("profile/assembly/{sample}_mapped_read.txt",sample=R1),
           total = expand("%s/{sample}/{sample}_readsnb.txt"%IN, sample=R1)
    output:  "profile/mapping_percent.tsv"
    shell: """
            for path in {input.mapped}
            do
                file=$(basename $path)
                sample=${{file%_mapped_read.txt}}
                tot_reads=$(cat {IN}/$sample/$sample"_readsnb.txt")
                mapped_reads=$(cat $path)
                echo -e $sample\t$(bc -l <<< "$mapped_reads / $tot_reads")>>{output}
            done
           """







